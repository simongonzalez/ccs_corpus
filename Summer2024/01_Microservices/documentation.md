# Venezuelan Spanish Linguistic Characterization Project

## Introduction

Our work aims to extend technological innovations in linguistic characterization to enhance the understanding of Venezuelan Spanish. In the current phase of the project, we focus on converting the microservices from R to Python. We aim to migrate microservices responsible for transcription mapping, natural language processing, forced alignment, acoustic extraction, and data merging to Python. With this conversion, we hope to streamline the workflow, leveraging more advanced libraries and tools available in the Python ecosystem. This will improve the efficiency and accuracy of our speech-to-text processing, forced alignment, and acoustic feature extraction. The methodology for each microservice is described below in detail.

---
![Alt text](https://github.com/simongonzalez/ccs_corpus/blob/main/Summer2024/01_Microservices/documentation/flow_chart.png)

## Microservices

### Speech to Text Analysis

**Description**:  
This script utilizes OpenAI's Whisper model to convert audio files in MP3 format into subtitle files in the SRT format. It processes each audio file, transcribes the spoken content, and generates a time-stamped SRT file suitable for use in video subtitles or for creating transcripts.

**Inputs/Outputs**:  
- **Inputs**: Audio files in MP3 format.  
- **Outputs**: Subtitle files in SRT format, which include time-stamped transcriptions of the audio content.

**Tools/Technology Used**:  
- **Transcription Model**: OpenAI Whisper  
- **Specific Python Libraries**: `pydub`, `librosa`

**Limitations**:  
- The link to the GitHub repository in the Colab document needs to be changed.

---

### Transcription Mapping

**Description**:  
This script processes manual transcripts and corresponding Whisper-generated subtitle files to generate n-gram data and match segments of spoken language with their textual representation.

**Inputs/Outputs**:  
- **Inputs**: Manual transcripts in .doc format, corresponding .srt files generated by Whisper.  
- **Outputs**: Matched n-gram data saved as CSV files for different n-gram lengths (2 to 10). TextGrid file containing intervals for each identified spoken segment.

**Tools/Technology Used**:  
- **Specific Python Libraries**: `parselmouth`, `tgt`, `pypandoc`, `pysrt`, `libreoffice`, `nltk`

**Limitations**:  
- Assumes manual transcripts are in .doc format; other formats will not be processed.
- The script's handling of different file encodings may not always be correct. 
- The script was tested only in a Mac environment.

---

### Natural Language Processing (NLP)

**Description**:  
The script processes Spanish manual transcriptions from .doc files, cleaning the text, identifying question and exclamation structures, and annotating the text using the Stanza NLP library. It generates linguistic statistics and speaker-specific metrics.

**Inputs/Outputs**:  
- **Inputs**: Manual transcripts in .doc format.  
- **Outputs**: CSV file containing cleaned, segmented text and various analysis data.

**Tools/Technology Used**:  
- **Specific Python Libraries**: `stanza`, `pypandoc`

**Limitations**:  
- Only supports .doc/.docx files; additional conversion is required for other formats.
- Performance may degrade with large datasets, especially due to NLP processing bottlenecks.

---

### Montreal Forced Alignment (MFA)

**Description**:  
This script processes TextGrid files to extract and analyze phonetic segment data, including creating and merging DataFrames, processing intervals, and extracting segment-related information.

**Inputs/Outputs**:  
- **Inputs**: TextGrid files before and after processing with Montreal Forced Aligner.  
- **Outputs**: CSV files (df_phon.csv and dat_append.csv) containing segment-related information and phonological features.

**Tools/Technology Used**:  
- **Specific Python Libraries**: `praatio`, `pandas`, `seaborn`

**Limitations**:  
- Relies on the accuracy and consistency of the input TextGrid files.

---

### Acoustic Extraction

**Description**:  
The script processes audio interview files and corresponding CSV data files, extracting audio features such as MFCCs, pitch, intensity, and formants.

**Inputs/Outputs**:  
- **Inputs**: MP3 interview audio files, corresponding CSV files with speaker and timestamp information.  
- **Outputs**: Updated CSV files with extracted audio features.

**Tools/Technology Used**:  
- **Specific Python Libraries**: `parselmouth`, `librosa`

**Limitations**:  
- Processing can be slow for large audio files or CSVs with many rows.

---

### Data Merging

**Description**:  
The script processes formant data from vowels, removing outliers, normalizing and scaling the data, and performing visualizations for vowel space and duration distributions.

**Inputs/Outputs**:  
- **Inputs**: CSV files with extracted audio features, phonological contexts.  
- **Outputs**: Formant plots, vowel space plots, mean duration plots, and a CSV file containing normalized and rescaled formant frequencies with additional context.

**Tools/Technology Used**:  
- **Specific Python Libraries**: `scipy`, `seaborn`, `matplotlib`

**Limitations**:  
- The `find_outliers` function was replicated due to the absence of a similar Python library found in 'joeyr'.

---

## Conclusion

This project involves several scripts that handle various aspects of speech analysis, from transcription to linguistic analysis and data visualization. The conversion from R to Python aims to enhance the workflow's efficiency and accuracy, leveraging Python's advanced libraries and tools. Each microservice is carefully documented, detailing its functions, inputs, outputs, and any limitations.

For more information and access to the scripts, please refer to the respective sections above or explore the project repository.
