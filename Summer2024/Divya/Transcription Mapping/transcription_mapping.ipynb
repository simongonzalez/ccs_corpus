{
 "cells": [
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/pradeepchandran/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import os\n",
    "from itertools import islice\n",
    "import parselmouth\n",
    "from parselmouth.praat import call\n",
    "from nltk.util import ngrams\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "import chardet\n",
    "from docx import Document\n",
    "import tgt\n",
    "from subprocess import call\n",
    "import os\n",
    "import pypandoc"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-05T00:00:16.974728Z",
     "start_time": "2024-08-05T00:00:15.263618Z"
    }
   },
   "id": "30e842e91ebbe4d1",
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def detect_encoding(file_path):\n",
    "    with open(file_path, 'rb') as file:\n",
    "        raw_data = file.read()\n",
    "    result = chardet.detect(raw_data)\n",
    "    return result['encoding']"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-05T00:00:26.379290Z",
     "start_time": "2024-08-05T00:00:26.369978Z"
    }
   },
   "id": "4e67507dc8996136",
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def read_doc(file_path):\n",
    "    doc = Document(file_path)\n",
    "    lines_all = []\n",
    "    for paragraph in doc.paragraphs:\n",
    "        # Split the text of the paragraph into lines\n",
    "        lines = paragraph.text.split('\\n')\n",
    "        # Iterate through each line in the paragraph\n",
    "        for line in lines:\n",
    "            lines_all.append(line)\n",
    "    return lines_all"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-05T00:00:33.329413Z",
     "start_time": "2024-08-05T00:00:33.322089Z"
    }
   },
   "id": "10526e1c8d721337",
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    try:\n",
    "        return text.encode('utf-8').decode('utf-8')\n",
    "    except UnicodeDecodeError:\n",
    "        return None"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-05T00:00:39.349556Z",
     "start_time": "2024-08-05T00:00:39.341935Z"
    }
   },
   "id": "e72568b721badee4",
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def split_text(row):\n",
    "    pattern = r'\\.\\.\\.|/TEXT/|//|\\.|/|;'\n",
    "    split_texts = re.split(pattern, row['text'])\n",
    "    # Remove empty strings\n",
    "    split_texts = [text for text in split_texts if text.strip() != '']\n",
    "    return pd.DataFrame({\n",
    "        'raw': row['raw'],\n",
    "        'speaker': row['speaker'],\n",
    "        'text': split_texts\n",
    "    })"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-05T00:00:47.022103Z",
     "start_time": "2024-08-05T00:00:46.999326Z"
    }
   },
   "id": "d8be6e983f32c947",
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def determine_speaker(text):\n",
    "    if re.search(r'^(Habl|HABL|I\\.|I:|O:)', text):\n",
    "        return 'Hablante'\n",
    "    elif re.search(r'^(Enc\\.:|Enc\\.[0-9]:|E\\.|E[0-9]:|AUX[0-9]:)', text):\n",
    "        return 'Entrevistador'\n",
    "    else:\n",
    "        return None"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-05T00:00:53.440059Z",
     "start_time": "2024-08-05T00:00:53.433475Z"
    }
   },
   "id": "9851a530ff799973",
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def convert_to_docx_if_needed(input_file):\n",
    "    output_file = os.path.splitext(input_file)[0] + '.docx'\n",
    "\n",
    "    if os.path.exists(output_file):\n",
    "        print(f\"{output_file} already exists. Skipping conversion.\")\n",
    "    else:\n",
    "        try:\n",
    "            pypandoc.convert_file(input_file, 'docx', outputfile=output_file)\n",
    "            print(f\"File converted and saved to {output_file}\")\n",
    "        except RuntimeError:\n",
    "            output_path = os.path.dirname(input_file)\n",
    "            call(['libreoffice', '--convert-to', 'docx', input_file, '--outdir', output_path])\n",
    "            print(f\"File converted and saved to {output_file}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-05T00:01:02.115219Z",
     "start_time": "2024-08-05T00:01:02.107153Z"
    }
   },
   "id": "2b5b7c759a01562f",
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def main():\n",
    "    current_working_directory = os.getcwd()\n",
    "    # Read the CSV file\n",
    "    duration_file_path = os.path.join(current_working_directory, 'Input_Output', 'Input',\n",
    "                                      'XX_duration_files_20240324.csv')\n",
    "    fls_df = pd.read_csv(duration_file_path)\n",
    "\n",
    "    # List all .doc files in the directory\n",
    "    manual_transcript_file_path = os.path.join(current_working_directory, 'Input_Output', 'Input')\n",
    "\n",
    "    fls_doc_original = [os.path.join(manual_transcript_file_path, f) for f in os.listdir(manual_transcript_file_path) if\n",
    "               f.endswith('.doc')]\n",
    "\n",
    "    for file in fls_doc_original:\n",
    "        convert_to_docx_if_needed(file)\n",
    "\n",
    "    fls_doc = [os.path.join(manual_transcript_file_path, f) for f in os.listdir(manual_transcript_file_path) if\n",
    "               f.endswith('.docx')]\n",
    "\n",
    "    #current_working_directory = os.getcwd()\n",
    "\n",
    "    # List all .srt files in the directory\n",
    "    srt_file_path = os.path.join(current_working_directory, 'Input_Output', 'Input')\n",
    "    fls_son = [os.path.join(srt_file_path, f) for f in os.listdir(srt_file_path)\n",
    "               if f.endswith('.srt')]\n",
    "\n",
    "    # Create a DataFrame for .srt files\n",
    "    dfall_son = pd.DataFrame({\n",
    "        'srt': fls_son,\n",
    "        'file_srt': [os.path.basename(f) for f in fls_son],\n",
    "        'name': [re.sub(r'\\.mp3\\.srt$', '', os.path.basename(f)) for f in fls_son]\n",
    "    })\n",
    "\n",
    "    # Create a DataFrame for .doc files and merge with dfall_son and fls_df\n",
    "    dfall = pd.DataFrame({\n",
    "        'doc': fls_doc,\n",
    "        'file_doc': [os.path.basename(f) for f in fls_doc],\n",
    "        'name': [re.sub(r'\\.docx$', '', os.path.basename(f)) for f in fls_doc]\n",
    "    }).merge(dfall_son, on='name', how='left').merge(fls_df, on='name', how='left').dropna(subset=['srt'])\n",
    "\n",
    "    output_directory = os.path.join(current_working_directory, 'Input_Output', 'Output', 'XX_TG_Matched')\n",
    "    os.makedirs(output_directory, exist_ok=True)\n",
    "\n",
    "    df_doc_output_directory = os.path.join(output_directory, 'df_doc_ngrams')\n",
    "    os.makedirs(df_doc_output_directory, exist_ok=True)\n",
    "    for index, file in dfall.iterrows():\n",
    "        df_doc = pd.DataFrame({'raw': [line.strip()\n",
    "                                       for line in read_doc(file['doc'])\n",
    "                                       if line.strip()]})\n",
    "\n",
    "        pattern = r'\\.\\.\\.|/TEXT/|//|\\.|/|;'\n",
    "        # create a speaker column and fill it downwards\n",
    "        # df_doc['speaker'] = df_doc['raw'].apply(lambda x: 'Hablante' if re.match(r'^Habl', x)\n",
    "        # else ('Entrevistador' if re.match(r'^Enc\\.\\d:', x) else None))\n",
    "        df_doc['speaker'] = df_doc['raw'].apply(determine_speaker)\n",
    "        df_doc['speaker'] = df_doc['speaker'].fillna(method='ffill')\n",
    "        df_doc = df_doc.dropna(subset=['speaker'])\n",
    "\n",
    "        # clean the text column\n",
    "        # df_doc['text'] = df_doc['raw'].str.replace(r'Habl.:|Habl.|Habl:|Enc.\\d:|Enc.', '', regex=True).str.strip().str.lower()\n",
    "        df_doc['text'] = df_doc['raw'].str.replace(r'Habl.:|Habl.|Habl:|Enc.\\d:|Enc.:|Enc.|HABL:|I\\.:|E\\.:|E[0-9]:|AUX1:|O:', ''\n",
    "                                                   , regex=True)\n",
    "        df_doc_temp = pd.concat(df_doc.apply(split_text, axis=1).values)\n",
    "        df_doc = df_doc_temp\n",
    "        df_doc['text'] = df_doc['text'].str.lower()\n",
    "        df_doc['text'] = df_doc['text'].str.replace(r'[^\\w\\s]', '', regex=True)\n",
    "        df_doc['text'] = df_doc['text'].str.replace(r\" pa' \", ' para ', regex=False)\n",
    "        df_doc['text'] = df_doc['text'].str.replace(r\"nadien\", 'nadie', regex=False)\n",
    "        df_doc = df_doc.dropna(subset=['text'])\n",
    "        df_doc = df_doc[df_doc['text'].str.strip() != '']\n",
    "        # df_doc['text'] = df_doc['text'].str.replace(r'[,\\.\\?\\¿¡\\[\\]\\\"]|\\.{3}', '', regex=True)\n",
    "\n",
    "        df_doc['line_number_n'] = range(1, len(df_doc) + 1)\n",
    "\n",
    "\n",
    "        print(df_doc)\n",
    "\n",
    "        # generate ngrams from the manual transcription\n",
    "        def generate_ngrams(text, n):\n",
    "            words = word_tokenize(text)\n",
    "            return [' '.join(grams) for grams in nltk.ngrams(words, n)]\n",
    "\n",
    "\n",
    "        df_doc['ngram_text'] = df_doc['text'].apply(lambda x: generate_ngrams(x, 10))\n",
    "        df_doc_ngram_10 = df_doc.explode('ngram_text').dropna(subset=['ngram_text'])\n",
    "        print(os.path.join(output_directory, file['name'] + '_ngram_10'))\n",
    "        df_doc_ngram_10.to_csv(os.path.join(df_doc_output_directory, file['name'] + '_ngram_10.csv'), index=False)\n",
    "\n",
    "        df_doc['ngram_text'] = df_doc['text'].apply(lambda x: generate_ngrams(x, 9))\n",
    "        df_doc_ngram_9 = df_doc.explode('ngram_text').dropna(subset=['ngram_text'])\n",
    "        df_doc_ngram_9.to_csv(os.path.join(df_doc_output_directory, file['name'] + '_ngram_9.csv'), index=False)\n",
    "\n",
    "        df_doc['ngram_text'] = df_doc['text'].apply(lambda x: generate_ngrams(x, 8))\n",
    "        df_doc_ngram_8 = df_doc.explode('ngram_text').dropna(subset=['ngram_text'])\n",
    "        df_doc_ngram_8.to_csv(os.path.join(df_doc_output_directory, file['name'] + '_ngram_8.csv'), index=False)\n",
    "\n",
    "        df_doc['ngram_text'] = df_doc['text'].apply(lambda x: generate_ngrams(x, 7))\n",
    "        df_doc_ngram_7 = df_doc.explode('ngram_text').dropna(subset=['ngram_text'])\n",
    "        df_doc_ngram_7.to_csv(os.path.join(df_doc_output_directory, file['name'] + '_ngram_7.csv'), index=False)\n",
    "\n",
    "        df_doc['ngram_text'] = df_doc['text'].apply(lambda x: generate_ngrams(x, 6))\n",
    "        df_doc_ngram_6 = df_doc.explode('ngram_text').dropna(subset=['ngram_text'])\n",
    "        df_doc_ngram_6.to_csv(os.path.join(df_doc_output_directory, file['name'] + '_ngram_6.csv'), index=False)\n",
    "\n",
    "        df_doc['ngram_text'] = df_doc['text'].apply(lambda x: generate_ngrams(x, 5))\n",
    "        df_doc_ngram_5 = df_doc.explode('ngram_text').dropna(subset=['ngram_text'])\n",
    "        df_doc_ngram_5.to_csv(os.path.join(df_doc_output_directory, file['name'] + '_ngram_5.csv'), index=False)\n",
    "\n",
    "        df_doc['ngram_text'] = df_doc['text'].apply(lambda x: generate_ngrams(x, 4))\n",
    "        df_doc_ngram_4 = df_doc.explode('ngram_text').dropna(subset=['ngram_text'])\n",
    "        df_doc_ngram_4.to_csv(os.path.join(df_doc_output_directory, file['name'] + '_ngram_4.csv'), index=False)\n",
    "\n",
    "        df_doc['ngram_text'] = df_doc['text'].apply(lambda x: generate_ngrams(x, 3))\n",
    "        df_doc_ngram_3 = df_doc.explode('ngram_text').dropna(subset=['ngram_text'])\n",
    "        df_doc_ngram_3.to_csv(os.path.join(df_doc_output_directory, file['name'] + '_ngram_3.csv'), index=False)\n",
    "\n",
    "        df_doc['ngram_text'] = df_doc['text'].apply(lambda x: generate_ngrams(x, 2))\n",
    "        df_doc_ngram_2 = df_doc.explode('ngram_text').dropna(subset=['ngram_text'])\n",
    "        df_doc_ngram_2.to_csv(os.path.join(df_doc_output_directory, file['name'] + '_ngram_2.csv'), index=False)\n",
    "\n",
    "        # reading and pre-processing Sonix Transcripts\n",
    "        from pysrt import open as open_srt\n",
    "\n",
    "        def read_srt(file_path):\n",
    "            subs = open_srt(file_path)\n",
    "            return [{'n': sub.index, 'start': sub.start.ordinal, 'end': sub.end.ordinal, 'text': sub.text} for sub in subs]\n",
    "\n",
    "\n",
    "        dfson = pd.DataFrame(read_srt(file['srt']))\n",
    "        dfson['text'] = dfson['text'].str.replace(r'SPEAKER\\d\\:|[,.\\?!¿…!\\[\\]\"¡]', '', regex=True).str.strip().str.lower()\n",
    "        dfson['word_n'] = dfson['text'].apply(lambda x: len(re.findall(r'\\w+', x)))\n",
    "        dfson = dfson[dfson['word_n'] >= 5]\n",
    "        text_counts = dfson['text'].value_counts().reset_index()\n",
    "        text_counts.columns = ['text', 'text_unique']\n",
    "        dfson = dfson.merge(text_counts, on='text')\n",
    "        dfson = dfson[dfson['text_unique'] == 1]\n",
    "\n",
    "        dfson['ngram_text'] = dfson['text'].apply(lambda x: generate_ngrams(x, 10))\n",
    "        dfson_ngram_10 = dfson.explode('ngram_text').dropna(subset=['ngram_text'])\n",
    "        dfson_ngram_10 = dfson_ngram_10[['n', 'start', 'end', 'ngram_text']]\n",
    "\n",
    "        dfson['ngram_text'] = dfson['text'].apply(lambda x: generate_ngrams(x, 9))\n",
    "        dfson_ngram_9 = dfson.explode('ngram_text').dropna(subset=['ngram_text'])\n",
    "        dfson_ngram_9 = dfson_ngram_9[['n', 'start', 'end', 'ngram_text']]\n",
    "\n",
    "        dfson['ngram_text'] = dfson['text'].apply(lambda x: generate_ngrams(x, 8))\n",
    "        dfson_ngram_8 = dfson.explode('ngram_text').dropna(subset=['ngram_text'])\n",
    "        dfson_ngram_8 = dfson_ngram_8[['n', 'start', 'end', 'ngram_text']]\n",
    "\n",
    "        dfson['ngram_text'] = dfson['text'].apply(lambda x: generate_ngrams(x, 7))\n",
    "        dfson_ngram_7 = dfson.explode('ngram_text').dropna(subset=['ngram_text'])\n",
    "        dfson_ngram_7 = dfson_ngram_7[['n', 'start', 'end', 'ngram_text']]\n",
    "\n",
    "        dfson['ngram_text'] = dfson['text'].apply(lambda x: generate_ngrams(x, 6))\n",
    "        dfson_ngram_6 = dfson.explode('ngram_text').dropna(subset=['ngram_text'])\n",
    "        dfson_ngram_6 = dfson_ngram_6[['n', 'start', 'end', 'ngram_text']]\n",
    "\n",
    "        dfson['ngram_text'] = dfson['text'].apply(lambda x: generate_ngrams(x, 5))\n",
    "        dfson_ngram_5 = dfson.explode('ngram_text').dropna(subset=['ngram_text'])\n",
    "        dfson_ngram_5 = dfson_ngram_5[['n', 'start', 'end', 'ngram_text']]\n",
    "\n",
    "        dfson['ngram_text'] = dfson['text'].apply(lambda x: generate_ngrams(x, 4))\n",
    "        dfson_ngram_4 = dfson.explode('ngram_text').dropna(subset=['ngram_text'])\n",
    "        dfson_ngram_4 = dfson_ngram_4[['n', 'start', 'end', 'ngram_text']]\n",
    "\n",
    "        dfson['ngram_text'] = dfson['text'].apply(lambda x: generate_ngrams(x, 3))\n",
    "        dfson_ngram_3 = dfson.explode('ngram_text').dropna(subset=['ngram_text'])\n",
    "        dfson_ngram_3 = dfson_ngram_3[['n', 'start', 'end', 'ngram_text']]\n",
    "\n",
    "        dfson['ngram_text'] = dfson['text'].apply(lambda x: generate_ngrams(x, 2))\n",
    "        dfson_ngram_2 = dfson.explode('ngram_text').dropna(subset=['ngram_text'])\n",
    "        dfson_ngram_2 = dfson_ngram_2[['n', 'start', 'end', 'ngram_text']]\n",
    "\n",
    "        # match manual and sonix transcripts\n",
    "        df_matched_10 = pd.merge(df_doc_ngram_10, dfson_ngram_10, on='ngram_text', how='left')\n",
    "        df_matched_10 = df_matched_10.dropna(subset=['n']).drop_duplicates(subset=['line_number_n']).assign(ngram_number=10)\n",
    "\n",
    "        df_matched_9 = pd.merge(df_doc_ngram_9, dfson_ngram_9, on='ngram_text', how='left')\n",
    "        df_matched_9 = df_matched_9.dropna(subset=['n']).drop_duplicates(subset=['line_number_n']).assign(ngram_number=9)\n",
    "\n",
    "        df_matched_8 = pd.merge(df_doc_ngram_8, dfson_ngram_8, on='ngram_text', how='left')\n",
    "        df_matched_8 = df_matched_8.dropna(subset=['n']).drop_duplicates(subset=['line_number_n']).assign(ngram_number=8)\n",
    "\n",
    "        df_matched_7 = pd.merge(df_doc_ngram_7, dfson_ngram_7, on='ngram_text', how='left')\n",
    "        df_matched_7 = df_matched_7.dropna(subset=['n']).drop_duplicates(subset=['line_number_n']).assign(ngram_number=7)\n",
    "\n",
    "        df_matched_6 = pd.merge(df_doc_ngram_6, dfson_ngram_6, on='ngram_text', how='left')\n",
    "        df_matched_6 = df_matched_6.dropna(subset=['n']).drop_duplicates(subset=['line_number_n']).assign(ngram_number=6)\n",
    "\n",
    "        df_matched_5 = pd.merge(df_doc_ngram_5, dfson_ngram_5, on='ngram_text', how='left')\n",
    "        df_matched_5 = df_matched_5.dropna(subset=['n']).drop_duplicates(subset=['line_number_n']).assign(ngram_number=5)\n",
    "\n",
    "        df_matched_4 = pd.merge(df_doc_ngram_4, dfson_ngram_4, on='ngram_text', how='left')\n",
    "        df_matched_4 = df_matched_4.dropna(subset=['n']).drop_duplicates(subset=['line_number_n']).assign(ngram_number=4)\n",
    "\n",
    "        df_matched_3 = pd.merge(df_doc_ngram_3, dfson_ngram_3, on='ngram_text', how='left')\n",
    "        df_matched_3 = df_matched_3.dropna(subset=['n']).drop_duplicates(subset=['line_number_n']).assign(ngram_number=3)\n",
    "\n",
    "        df_matched_2 = pd.merge(df_doc_ngram_2, dfson_ngram_2, on='ngram_text', how='left')\n",
    "        df_matched_2 = df_matched_2.dropna(subset=['n']).drop_duplicates(subset=['line_number_n']).assign(ngram_number=2)\n",
    "\n",
    "\n",
    "        for i in range(2, 11):\n",
    "            print(f'length at ngram {i} df_ngram, dfson, matched',eval(f'df_doc_ngram_{i}').shape,\n",
    "                  eval(f'dfson_ngram_{i}').shape, eval(f'df_matched_{i}').shape)\n",
    "\n",
    "\n",
    "        #matching all the dfs\n",
    "        matches_from_9 = set(df_matched_9['line_number_n']) - set(df_matched_10['line_number_n'])\n",
    "        df_matched_all = (pd.concat([df_matched_10, df_matched_9[df_matched_9['line_number_n'].isin(matches_from_9)]])\n",
    "                          .sort_values('line_number_n'))\n",
    "\n",
    "        matches_from_8 = set(df_matched_8['line_number_n']) - set(df_matched_all['line_number_n'])\n",
    "        df_matched_all = (pd.concat([df_matched_all, df_matched_8[df_matched_8['line_number_n'].isin(matches_from_8)]])\n",
    "                          .sort_values('line_number_n'))\n",
    "\n",
    "        matches_from_7 = set(df_matched_7['line_number_n']) - set(df_matched_all['line_number_n'])\n",
    "        df_matched_all = (pd.concat([df_matched_all, df_matched_7[df_matched_7['line_number_n'].isin(matches_from_7)]])\n",
    "                          .sort_values('line_number_n'))\n",
    "\n",
    "        matches_from_6 = set(df_matched_6['line_number_n']) - set(df_matched_all['line_number_n'])\n",
    "        df_matched_all = (pd.concat([df_matched_all, df_matched_6[df_matched_6['line_number_n'].isin(matches_from_6)]])\n",
    "                          .sort_values('line_number_n'))\n",
    "\n",
    "        matches_from_5 = set(df_matched_5['line_number_n']) - set(df_matched_all['line_number_n'])\n",
    "        df_matched_all = (pd.concat([df_matched_all, df_matched_5[df_matched_5['line_number_n'].isin(matches_from_5)]])\n",
    "                          .sort_values('line_number_n'))\n",
    "\n",
    "        matches_from_4 = set(df_matched_4['line_number_n']) - set(df_matched_all['line_number_n'])\n",
    "        df_matched_all = (pd.concat([df_matched_all, df_matched_4[df_matched_4['line_number_n'].isin(matches_from_4)]])\n",
    "                          .sort_values('line_number_n'))\n",
    "\n",
    "        df_speaker = df_matched_all[df_matched_all['speaker'] == 'Hablante']\n",
    "        df_speaker = df_speaker[~(df_speaker['end'] < df_speaker['start'])]\n",
    "\n",
    "        # for _ in range(10):\n",
    "        #     for rowi in range(len(df_speaker) - 1, 0, -1):\n",
    "        #         tmp_start = df_speaker.iloc[rowi]['start']\n",
    "        #         tmp_start_previous = df_speaker.iloc[rowi - 1]['start']\n",
    "        #         if tmp_start < tmp_start_previous:\n",
    "        #             df_speaker = df_speaker.drop(df_speaker.index[rowi])\n",
    "\n",
    "        df_speaker = df_speaker.drop_duplicates(subset=['start'])\n",
    "\n",
    "        # for rowi in range(len(df_speaker) - 1):\n",
    "        #     tmp_end = df_speaker.iloc[rowi]['end']\n",
    "        #     tmp_start_next = df_speaker.iloc[rowi + 1]['start']\n",
    "        #     if tmp_end > tmp_start_next:\n",
    "        #         df_speaker.at[df_speaker.index[rowi], 'end'] = tmp_start_next\n",
    "\n",
    "        tgdur = file['dur']\n",
    "\n",
    "        tmpspeakerdf = df_speaker[df_speaker['end'] > df_speaker['start']]\n",
    "        tmpspeakerdf['start'] = tmpspeakerdf['start'].astype(float) / 1000\n",
    "        tmpspeakerdf['end'] = tmpspeakerdf['end'].astype(float) / 1000\n",
    "        tmpspeakerdf = tmpspeakerdf[tmpspeakerdf['start'] <= tgdur]\n",
    "        tmpspeakerdf = tmpspeakerdf[tmpspeakerdf['end'] <= tgdur]\n",
    "        tmpspeakerdf = tmpspeakerdf.sort_values(by='start')\n",
    "\n",
    "\n",
    "        # df_speaker_out.to_csv('Output/SpeakingTurns_CA1HA_87.csv', index=False, sep=',', quoting=3)\n",
    "        # Create a TextGrid using parselmouth\n",
    "        tg = tgt.TextGrid()\n",
    "\n",
    "        # Create a new interval tier\n",
    "        tier = tgt.IntervalTier( 0, tgdur, 'Speaker')\n",
    "\n",
    "\n",
    "        for index, row in tmpspeakerdf.iterrows():\n",
    "            interval = tgt.Interval(row['start'], row['end'], row['text'])\n",
    "            tier.add_interval(interval)\n",
    "        #\n",
    "        save_name = os.path.join(output_directory, file['name'] + '.TextGrid')\n",
    "        # call(tg, \"Write to text file\", save_name)\n",
    "\n",
    "        tg.add_tier(tier)\n",
    "        tgt.io.write_to_file(tg, save_name)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-05T00:02:19.437052Z",
     "start_time": "2024-08-05T00:02:19.377515Z"
    }
   },
   "id": "9b84fdd7f1f087df",
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/pradeepchandran/ccs_corpus/Summer2024/Divya/Transcription Mapping/Input_Output/Input/CA1HA_87.docx already exists. Skipping conversion.\n",
      "                                                  raw        speaker  \\\n",
      "0   Habl. Ah, bueno, bueno, yo nací en Caracas el ...       Hablante   \n",
      "1   Habl. Ah, bueno, bueno, yo nací en Caracas el ...       Hablante   \n",
      "2   Habl. Ah, bueno, bueno, yo nací en Caracas el ...       Hablante   \n",
      "3   Habl. Ah, bueno, bueno, yo nací en Caracas el ...       Hablante   \n",
      "4   Habl. Ah, bueno, bueno, yo nací en Caracas el ...       Hablante   \n",
      "..                                                ...            ...   \n",
      "0   Enc.1: ¿Tuvistes algún conflicto allá con algu...  Entrevistador   \n",
      "0   Habl. Una vez me asaltaron, sí, y me quitaron ...       Hablante   \n",
      "1   Habl. Una vez me asaltaron, sí, y me quitaron ...       Hablante   \n",
      "2   Habl. Una vez me asaltaron, sí, y me quitaron ...       Hablante   \n",
      "3   Habl. Una vez me asaltaron, sí, y me quitaron ...       Hablante   \n",
      "\n",
      "                                                 text  line_number_n  \n",
      "0    ah bueno bueno yo nací en caracas el diez de ...              1  \n",
      "1                                                 eh               2  \n",
      "2             mis padres vivían en un apartamento en               3  \n",
      "3                 en las mercedes mientras estaba en               4  \n",
      "4    en proyecto de construcción la casa en la cas...              5  \n",
      "..                                                ...            ...  \n",
      "0    tuvistes algún conflicto allá con alguien por...            615  \n",
      "0              una vez me asaltaron sí y me quitaron             616  \n",
      "1      yo había ido estaba estudiando en cambridge y             617  \n",
      "2                                            fui a ve            618  \n",
      "3    iba a pasar el fin de semana a londres con tr...            619  \n",
      "\n",
      "[619 rows x 4 columns]\n",
      "/Users/pradeepchandran/ccs_corpus/Summer2024/Divya/Transcription Mapping/Input_Output/Output/XX_TG_Matched/CA1HA_87_ngram_10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/mk/d88d1t4d5w13rj554jcl_r5w0000gn/T/ipykernel_49381/4186266760.py:56: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  df_doc['speaker'] = df_doc['speaker'].fillna(method='ffill')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length at ngram 2 df_ngram, dfson, matched (4422, 5) (3913, 4) (472, 9)\n",
      "length at ngram 3 df_ngram, dfson, matched (3890, 5) (3487, 4) (380, 9)\n",
      "length at ngram 4 df_ngram, dfson, matched (3418, 5) (3061, 4) (310, 9)\n",
      "length at ngram 5 df_ngram, dfson, matched (2987, 5) (2635, 4) (252, 9)\n",
      "length at ngram 6 df_ngram, dfson, matched (2608, 5) (2209, 4) (191, 9)\n",
      "length at ngram 7 df_ngram, dfson, matched (2271, 5) (1826, 4) (133, 9)\n",
      "length at ngram 8 df_ngram, dfson, matched (1990, 5) (1498, 4) (101, 9)\n",
      "length at ngram 9 df_ngram, dfson, matched (1749, 5) (1206, 4) (74, 9)\n",
      "length at ngram 10 df_ngram, dfson, matched (1535, 5) (958, 4) (53, 9)\n"
     ]
    }
   ],
   "source": [
    "main()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-05T00:04:13.735123Z",
     "start_time": "2024-08-05T00:04:12.246474Z"
    }
   },
   "id": "6e1b20244a6dd39a",
   "execution_count": 14
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "633f757faf6c6cc3"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
